{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bfbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor # работает хуже\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b4152",
   "metadata": {},
   "source": [
    "<h2>Preprocessing the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f089ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    Функция предобработки признаков\n",
    "    Принимает pandas.DataFrame \n",
    "    Отдает pandas.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    # выбросим ненужные признаки, неинформативные/ухудшающие качество модели\n",
    "    cols = ['CustomerID', 'Count', \n",
    "            'Country', 'State', \n",
    "            'Lat Long', 'City']\n",
    "    df.drop(cols, axis=1, inplace = True)\n",
    "    \n",
    "    # обработка бинарных категориальных признаков\n",
    "    df['Gender'].replace({'Female':1,'Male':0},inplace=True)\n",
    "    df['Senior Citizen'].replace({'Yes':1,'No':0},inplace=True)\n",
    "    df['Partner'].replace({'Yes':1,'No':0},inplace=True)\n",
    "    df['Dependents'].replace({'Yes':1,'No':0},inplace=True)\n",
    "    df['Phone Service'].replace({'Yes':1,'No':0},inplace=True)\n",
    "    df['Paperless Billing'].replace({'Yes':1,'No':0},inplace=True)\n",
    "    \n",
    "    # попытка фиче генерации \n",
    "    # признак 'money':  сколько всего за все время клиент денег принес\n",
    "    df['money'] = df['Monthly Charges'] * df['Tenure Months'] \n",
    "    # признак 'dist':  расстояние от абстрактного центра (чтобы хоть как-то учесть местоположение)\n",
    "    df['dist'] = np.sqrt(df['Latitude'] ** 2 + df['Longitude']**2)\n",
    "    tmp  = ['Latitude', 'Longitude']\n",
    "    df.drop(tmp, axis=1, inplace = True)\n",
    "     \n",
    "    \n",
    "    # побьем кол-во месяцев на группы\n",
    "    def Tenure_Months_to_parts(x):\n",
    "        if x<=10:\n",
    "            return 'tenure_low'\n",
    "        elif 10<x<20:\n",
    "            return 'tenure_norm1'\n",
    "        elif 20<=x<45:\n",
    "            return 'tenure_norm2'\n",
    "        elif 45<=x<60:\n",
    "            return 'tenure_norm3'\n",
    "        else:\n",
    "            return 'tenure_high'\n",
    "        \n",
    "    df['Tenure Months'] = df['Tenure Months'].apply(Tenure_Months_to_parts)\n",
    "    \n",
    "    \n",
    "    #o-h-encoding категориальных\n",
    "    cols_ohe = ['Internet Service', 'Contract', \n",
    "                'Payment Method','Online Security',\n",
    "                'Online Backup','Tech Support',\n",
    "               'Streaming TV', 'Streaming Movies',\n",
    "               'Device Protection','Multiple Lines',\n",
    "                'Tenure Months', 'Paperless Billing']\n",
    "    \n",
    "    df = pd.get_dummies(data=df, columns= cols_ohe)\n",
    "    \n",
    "    # возникает очень много признаков скоррелированных и не информативных\n",
    "    # выбросим их\n",
    "    cols_to_drop = ['Online Security_No internet service',\n",
    "                    'Online Backup_No internet service',\n",
    "                    'Tech Support_No internet service',\n",
    "                    'Streaming TV_No internet service',\n",
    "                    'Streaming Movies_No internet service',\n",
    "                    'Device Protection_No internet service',\n",
    "                    'Multiple Lines_No phone service']\n",
    "    \n",
    "    df.drop(cols_to_drop, axis=1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3dbfc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7258d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kaggle_train_churn.csv')\n",
    "df = preprocessing(df)\n",
    "question_cols = ['Churn Score', 'CLTV', 'Churn Reason']\n",
    "df.drop(question_cols, axis=1, inplace = True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Churn_Value'] = df['Churn Value']\n",
    "df.drop(['Churn Value'],axis=1,inplace=True)\n",
    "\n",
    "# Скалируем данные\n",
    "scaler = MinMaxScaler()\n",
    "large_cols = list(df.columns[:-1])\n",
    "df[large_cols] = scaler.fit_transform(df[large_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264f20d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.74848\n",
      "[100]\tvalidation_0-auc:0.85014\n",
      "[178]\tvalidation_0-auc:0.84787\n",
      "[1]\tvalid_0's auc: 0.837406\tvalid_0's binary_logloss: 0.550292\n",
      "[2]\tvalid_0's auc: 0.83935\tvalid_0's binary_logloss: 0.5277\n",
      "[3]\tvalid_0's auc: 0.841711\tvalid_0's binary_logloss: 0.508793\n",
      "[4]\tvalid_0's auc: 0.843665\tvalid_0's binary_logloss: 0.49383\n",
      "[5]\tvalid_0's auc: 0.845363\tvalid_0's binary_logloss: 0.481146\n",
      "[6]\tvalid_0's auc: 0.849152\tvalid_0's binary_logloss: 0.469871\n",
      "[7]\tvalid_0's auc: 0.848237\tvalid_0's binary_logloss: 0.461252\n",
      "[8]\tvalid_0's auc: 0.849533\tvalid_0's binary_logloss: 0.453353\n",
      "[9]\tvalid_0's auc: 0.84973\tvalid_0's binary_logloss: 0.447009\n",
      "[10]\tvalid_0's auc: 0.850394\tvalid_0's binary_logloss: 0.441622\n",
      "[11]\tvalid_0's auc: 0.851054\tvalid_0's binary_logloss: 0.436574\n",
      "[12]\tvalid_0's auc: 0.852076\tvalid_0's binary_logloss: 0.431921\n",
      "[13]\tvalid_0's auc: 0.852156\tvalid_0's binary_logloss: 0.42846\n",
      "[14]\tvalid_0's auc: 0.851704\tvalid_0's binary_logloss: 0.42567\n",
      "[15]\tvalid_0's auc: 0.851629\tvalid_0's binary_logloss: 0.423026\n",
      "[16]\tvalid_0's auc: 0.851855\tvalid_0's binary_logloss: 0.42093\n",
      "[17]\tvalid_0's auc: 0.851364\tvalid_0's binary_logloss: 0.419747\n",
      "[18]\tvalid_0's auc: 0.850924\tvalid_0's binary_logloss: 0.418493\n",
      "[19]\tvalid_0's auc: 0.850873\tvalid_0's binary_logloss: 0.41713\n",
      "[20]\tvalid_0's auc: 0.850704\tvalid_0's binary_logloss: 0.416014\n",
      "[21]\tvalid_0's auc: 0.850184\tvalid_0's binary_logloss: 0.415357\n",
      "[22]\tvalid_0's auc: 0.850634\tvalid_0's binary_logloss: 0.414563\n",
      "[23]\tvalid_0's auc: 0.850946\tvalid_0's binary_logloss: 0.41352\n",
      "[24]\tvalid_0's auc: 0.849741\tvalid_0's binary_logloss: 0.413876\n",
      "[25]\tvalid_0's auc: 0.850207\tvalid_0's binary_logloss: 0.41297\n",
      "[26]\tvalid_0's auc: 0.850714\tvalid_0's binary_logloss: 0.412063\n",
      "[27]\tvalid_0's auc: 0.849586\tvalid_0's binary_logloss: 0.412696\n",
      "[28]\tvalid_0's auc: 0.849549\tvalid_0's binary_logloss: 0.412158\n",
      "[29]\tvalid_0's auc: 0.850079\tvalid_0's binary_logloss: 0.411233\n",
      "[30]\tvalid_0's auc: 0.850908\tvalid_0's binary_logloss: 0.410287\n",
      "[31]\tvalid_0's auc: 0.85031\tvalid_0's binary_logloss: 0.410609\n",
      "[32]\tvalid_0's auc: 0.850501\tvalid_0's binary_logloss: 0.410242\n",
      "[33]\tvalid_0's auc: 0.850588\tvalid_0's binary_logloss: 0.410177\n",
      "[34]\tvalid_0's auc: 0.850702\tvalid_0's binary_logloss: 0.409736\n",
      "[35]\tvalid_0's auc: 0.850819\tvalid_0's binary_logloss: 0.409711\n",
      "[36]\tvalid_0's auc: 0.850663\tvalid_0's binary_logloss: 0.409792\n",
      "[37]\tvalid_0's auc: 0.850001\tvalid_0's binary_logloss: 0.410586\n",
      "[38]\tvalid_0's auc: 0.849027\tvalid_0's binary_logloss: 0.411807\n",
      "[39]\tvalid_0's auc: 0.849173\tvalid_0's binary_logloss: 0.411767\n",
      "[40]\tvalid_0's auc: 0.848556\tvalid_0's binary_logloss: 0.412515\n",
      "[41]\tvalid_0's auc: 0.848515\tvalid_0's binary_logloss: 0.41262\n",
      "[42]\tvalid_0's auc: 0.847634\tvalid_0's binary_logloss: 0.413712\n",
      "[43]\tvalid_0's auc: 0.848351\tvalid_0's binary_logloss: 0.413152\n",
      "[44]\tvalid_0's auc: 0.848538\tvalid_0's binary_logloss: 0.412876\n",
      "[45]\tvalid_0's auc: 0.848178\tvalid_0's binary_logloss: 0.413244\n",
      "[46]\tvalid_0's auc: 0.847894\tvalid_0's binary_logloss: 0.413676\n",
      "[47]\tvalid_0's auc: 0.847821\tvalid_0's binary_logloss: 0.413659\n",
      "[48]\tvalid_0's auc: 0.847694\tvalid_0's binary_logloss: 0.414061\n",
      "[49]\tvalid_0's auc: 0.847315\tvalid_0's binary_logloss: 0.41409\n",
      "[50]\tvalid_0's auc: 0.847493\tvalid_0's binary_logloss: 0.414121\n",
      "[51]\tvalid_0's auc: 0.847278\tvalid_0's binary_logloss: 0.414467\n",
      "[52]\tvalid_0's auc: 0.846808\tvalid_0's binary_logloss: 0.415356\n",
      "[53]\tvalid_0's auc: 0.846876\tvalid_0's binary_logloss: 0.415727\n",
      "[54]\tvalid_0's auc: 0.846361\tvalid_0's binary_logloss: 0.416334\n",
      "[55]\tvalid_0's auc: 0.846196\tvalid_0's binary_logloss: 0.416739\n",
      "[56]\tvalid_0's auc: 0.84605\tvalid_0's binary_logloss: 0.416876\n",
      "[57]\tvalid_0's auc: 0.845904\tvalid_0's binary_logloss: 0.417376\n",
      "[58]\tvalid_0's auc: 0.845657\tvalid_0's binary_logloss: 0.417673\n",
      "[59]\tvalid_0's auc: 0.84576\tvalid_0's binary_logloss: 0.417717\n",
      "[60]\tvalid_0's auc: 0.845043\tvalid_0's binary_logloss: 0.418826\n",
      "[61]\tvalid_0's auc: 0.844286\tvalid_0's binary_logloss: 0.419565\n",
      "[62]\tvalid_0's auc: 0.844719\tvalid_0's binary_logloss: 0.419355\n",
      "[63]\tvalid_0's auc: 0.844459\tvalid_0's binary_logloss: 0.419707\n",
      "[64]\tvalid_0's auc: 0.844162\tvalid_0's binary_logloss: 0.4202\n",
      "[65]\tvalid_0's auc: 0.843879\tvalid_0's binary_logloss: 0.420639\n",
      "[66]\tvalid_0's auc: 0.843519\tvalid_0's binary_logloss: 0.421184\n",
      "[67]\tvalid_0's auc: 0.843149\tvalid_0's binary_logloss: 0.421708\n",
      "[68]\tvalid_0's auc: 0.843505\tvalid_0's binary_logloss: 0.421237\n",
      "[69]\tvalid_0's auc: 0.843532\tvalid_0's binary_logloss: 0.421402\n",
      "[70]\tvalid_0's auc: 0.842998\tvalid_0's binary_logloss: 0.42213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8540188639621624"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'Churn_Value'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "\n",
    "# оптимальные гиперпараметры для xgboost, random forest\n",
    "# для очевидных параметров типа 'max_depth', 'n_estimators' применялся GridSearch\n",
    "# для остальных optuna (который по сути как RandomizeSearch)\n",
    "\n",
    "param_x = {'alpha': 8,\n",
    " 'colsample_bytree': 0.5,\n",
    " 'eta': 0.5,\n",
    " 'gamma': 119,\n",
    " 'max_delta_step': 9,\n",
    " 'max_depth': 6,\n",
    " 'min_child_weight': 2,\n",
    " 'n_estimators': 967,\n",
    " 'refresh_leaf': 1,\n",
    " 'scale_pos_weight': 77,\n",
    " 'subsample': 1.0,\n",
    "  'random_state':42,\n",
    "          'use_label_encoder':False}\n",
    "\n",
    "\n",
    "params={'bootstrap': 'True',\n",
    " 'criterion': 'entropy',\n",
    " 'max_depth': 3625,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_leaf_nodes': 128,\n",
    " 'min_samples_leaf': 5,\n",
    " 'min_samples_split': 8,\n",
    " 'n_estimators': 441,\n",
    "       'random_state':0}\n",
    "\n",
    "model_1 = RandomForestClassifier(**params)\n",
    "model_x = XGBClassifier(**param_x)\n",
    "model_lgbm = LGBMClassifier(max_depth=7,n_estimators=70)\n",
    "\n",
    "\n",
    "model_2 = Pipeline(steps=[(\"classifier\",model_x)])\n",
    "model_3 = Pipeline(steps=[(\"classifier\",model_lgbm)])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_1.fit(X_train.values, y_train.values)\n",
    "\n",
    "\n",
    "model_2.fit(X_train, y_train,\n",
    "          classifier__eval_set=[(X_test, y_test)],\n",
    "          classifier__eval_metric =[\"auc\"],\n",
    "          classifier__verbose=100,\n",
    "          classifier__early_stopping_rounds=100)\n",
    "\n",
    "model_3.fit(X_train, y_train,\n",
    "          classifier__eval_set=[(X_test, y_test)],\n",
    "          classifier__eval_metric =[\"auc\"])\n",
    "\n",
    "\n",
    "preds_1 = model_1.predict_proba(X_test.values)\n",
    "preds_2 = model_2.predict_proba(X_test)\n",
    "preds_3 = model_3.predict_proba(X_test)\n",
    " \n",
    "roc_auc_score(y_test, preds_1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fa4e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('kaggle_test_churn.csv')\n",
    "df_test = preprocessing(df_test)\n",
    "df_test = scaler.transform(df_test)\n",
    "\n",
    "preds1=np.array(model_1.predict_proba(df_test))\n",
    "preds2=np.array(model_2.predict_proba(df_test))\n",
    "preds3=np.array(model_3.predict_proba(df_test))\n",
    "\n",
    "\n",
    "# параметры весов моделей подобраны руками\n",
    "preds = preds1*0.4 + preds2*0.3 + preds3*0.3\n",
    "\n",
    "\n",
    "test_submission_4 = pd.DataFrame(data={'Id':[i for i in range(1761)],'Predicted': preds[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04f59f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compression_opts = dict(method='zip',\n",
    "#                        archive_name='final4.csv')  \n",
    "#test_submission_4.to_csv('final4.zip', index=False,\n",
    "#          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba08351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# варианты параметров (хуже)\n",
    "param_x = {'alpha': 8,\n",
    " 'colsample_bytree': 0.5,\n",
    " 'eta': 0.5,\n",
    " 'gamma': 119,\n",
    " 'max_delta_step': 9,\n",
    " 'max_depth': 6,\n",
    " 'min_child_weight': 2,\n",
    " 'n_estimators': 967,\n",
    " 'refresh_leaf': 1,\n",
    " 'scale_pos_weight': 77,\n",
    " 'subsample': 1.0,\n",
    "  'random_state':42} \n",
    "params_x={'alpha': 10,\n",
    " 'colsample_bytree': 0.5,\n",
    " 'eta': 0.1,\n",
    " 'gamma': 76,\n",
    " 'max_delta_step': 2,\n",
    " 'max_depth': 13,\n",
    " 'min_child_weight': 16,\n",
    " 'n_estimators': 602,\n",
    " 'refresh_leaf': 0,\n",
    " 'scale_pos_weight': 34,\n",
    " 'subsample': 1.0}\n",
    "    \n",
    "    \n",
    "param ={'bootstrap': 'False',\n",
    " 'criterion': 'entropy',\n",
    " 'max_depth': 4153,\n",
    " 'max_features': 'auto',\n",
    " 'max_leaf_nodes': 1410,\n",
    " 'min_samples_leaf': 8,\n",
    " 'min_samples_split': 4,\n",
    " 'n_estimators': 88,\n",
    "  'random_state':2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1846a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
